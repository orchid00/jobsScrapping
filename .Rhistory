# Create running total dataframe
running <- data.frame(skill = KEYWORDS, count = rep(0, length(KEYWORDS)))
running
# Since the indeed only display max of 20 pages from search result, we cannot use job_count but need to track by creating a num_jobs
num_jobs <- 0
# Here is our results object that contains the two stats
results <- list("running" = running, "num_jobs" = num_jobs)
if (job_count != 0) {
cat("Scraping jobs in Start Page\n")
results <- ScrapeJobLinks(results, links)
}
source('~/Documents/RProjects/jobsScrapping/scripts/words.R')
source('~/Documents/RProjects/jobsScrapping/scripts/words.R')
source('~/Documents/RProjects/jobsScrapping/scripts/words.R')
source('~/Documents/RProjects/jobsScrapping/reports/words.R')
plotWords(mywords, "1")
url <- "https://www.indeed.com/rc/clk?jk=788e7e311656fc54&fccid=09fad757f3449fa5&vjs=3"
page_content <- getPageContent(url, 54)
page_content
# getPageSentences(page_content, "1")
getPageWords(page_content, "1")
# getPageSentences(page_content, "1")
mywords <- getPageWords(page_content, "1")
myw
mywords
# getPageSentences(page_content, "1")
mywords <- getPageWords(page_content, "1")
words_counts <-
page_content %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
group_by(word) %>%
count(sort = TRUE) %>%
ungroup()
words_counts
write_csv(words_counts, paste0("../results/words/", nam, ".csv"))
write_csv(words_counts, paste0("../results/words/", "1", ".csv"))
write_csv(words_counts, paste0("../results/words", nam, ".csv"))
write_csv(words_counts, paste0("../results/words", nam, ".csv"))
write_csv(words_counts, paste0("../results/words", "1", ".csv"))
write_csv(words_counts, paste0("../results/words", "1", ".csv"))
write_csv(words_counts, paste0("../../results/words", "1", ".csv"))
write_csv(words_counts, path = "results/words/1.csv")
getPageWords <- function(page_content, nam){
words_counts <-
page_content %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
group_by(word) %>%
count(sort = TRUE) %>%
ungroup()
write_csv(words_counts, paste0("results/words/", nam, ".csv"))
return(words_counts)
}
# getPageSentences(page_content, "1")
mywords <- getPageWords(page_content, "1")
getPageSentences <- function(page_content, nam){
collected_sentences <-
page_content %>%
unnest_tokens(sentence, text, token = "sentences")
write_csv(collected_sentences, paste0("results/sentences/", nam, ".csv"))
#return(collected_sentences)
}
getPageSentences(page_content, "1")
plotWords(mywords, "1")
plotWords <- function(words_counts, nam){
words_counts %>%
top_n(10) %>%
ggplot(aes(x = fct_reorder(word, n), y = n)) +
geom_bar(stat = "identity", width = 0.5) +
xlab(NULL) +
coord_flip() +
ylab("Word Frequency") +
ggtitle("Most Common Corpus Words") +
theme_minimal() +
theme(legend.position = "none",
text = element_text(size = 16, family = "serif"))
ggsave(filename = paste0("results/figures/", nam, ".png"))
}
plotWords(mywords, "1")
source('~/Documents/RProjects/jobsScrapping/reports/words.R')
# Get start page job URLs
links <- start_page %>%
html_nodes("h2 a") %>%
html_attr("href")
# Get result page links
page_links <- start_page %>%
html_nodes(xpath = '//div[contains(@class,"pagination")]//a') %>%
html_attr("href")
if_else(length(page_links) == 0, "Pages available: 0", paste("Pages available: ", length(page_links)))
KEYWORDS <- c("creation", "create", "communicate", "communicating",
"communication", "manage", "manages", "management", "SAS",
"Excel", "Java", "Tableau", "SQL", "database", "improvement",
"KPI", "control", "MSc", "Ph.D.",
"PhD", "experience", "tools", "systems", "autonomous", "methodical",
"organized", "organised", "Strong analytical",
"interpersonal skills", "adapts", "challenge", "challenges",
"continuous", "data management", "programming", "R", "Python",
"scripting", "leadership", "motivating")
# Clean the raw html - removing commas, tabs, line changers, etc
clean.text <- function(text) {
str_replace_all(text, regex("\r\n|\n|\t|\r|,|/|<|>|\\."), " ")
}
# Given running total dataframe and links to scrape skills and compute running total
ScrapeJobLinks <- function(res, job_links) {
for (i in seq_along(job_links)) {
job_url <- paste0(base_URL, job_links[i])
Sys.sleep(1)
cat(paste0("Reading job ", i, "\n"))
tryCatch({
page_content <- getPageContent(url, i)
getPageSentences(page_content, as.character(i))
mywords <- getPageWords(page_content, as.character(i))
plotWords(mywords, as.character(i))
df <- data.frame(skill = KEYWORDS, count = ifelse(str_detect(text, KEYWORDS), 1, 0))
res$running$count <- res$running$count + df$count
res$num_jobs <- res$num_jobs + 1
}, error = function(e) {
cat("ERROR :", conditionMessage(e), "\n")
})
}
return(res)
}
# Create running total dataframe
running <- data.frame(skill = KEYWORDS, count = rep(0, length(KEYWORDS)))
# Since the indeed only display max of 20 pages from search result, we cannot use job_count but need to track by creating a num_jobs
num_jobs <- 0
# Here is our results object that contains the two stats
results <- list("running" = running, "num_jobs" = num_jobs)
if (job_count != 0) {-
cat("Scraping jobs in Start Page\n")
results <- ScrapeJobLinks(results, links)
}
if (job_count != 0){
cat("Scraping jobs in Start Page\n")
results <- ScrapeJobLinks(results, links)
}
source(file = "words.R")
if (job_count != 0) {
cat("Scraping jobs in Start Page\n")
results <- ScrapeJobLinks(results, links)
}
job_url
i <- 1
job_url <- paste0(base_URL, job_links[i])
job_url <- paste0(base_URL, links[i])
job_url
Sys.sleep(1)
cat(paste0("Reading job ", i, "\n"))
page_content <- getPageContent(job_url, i)
getPageSentences(page_content, as.character(i))
mywords <- getPageWords(page_content, as.character(i))
mywords
plotWords(mywords, as.character(i))
# Clean the raw html - removing commas, tabs, line changers, etc
clean.text <- function(text) {
str_replace_all(text, regex("\r\n|\n|\t|\r|,|/|<|>|\\."), " ")
}
# Given running total dataframe and links to scrape skills and compute running total
ScrapeJobLinks <- function(res, job_links) {
for (i in seq_along(job_links)) {
job_url <- paste0(base_URL, job_links[i])
Sys.sleep(1)
cat(paste0("Reading job ", i, "\n"))
tryCatch({
page_content <- getPageContent(job_url, i)
getPageSentences(page_content, as.character(i))
mywords <- getPageWords(page_content, as.character(i))
plotWords(mywords, as.character(i))
df <- data.frame(skill = KEYWORDS, count = ifelse(str_detect(text, KEYWORDS), 1, 0))
res$running$count <- res$running$count + df$count
res$num_jobs <- res$num_jobs + 1
}, error = function(e) {
cat("ERROR :", conditionMessage(e), "\n")
})
}
return(res)
}
if (job_count != 0) {
cat("Scraping jobs in Start Page\n")
results <- ScrapeJobLinks(results, links)
}
plotWords(mywords, as.character(i))
df <- data.frame(skill = KEYWORDS, count = ifelse(str_detect(text, KEYWORDS), 1, 0))
mywords
text <- mywords$word
df <- data.frame(skill = KEYWORDS, count = ifelse(str_detect(text, KEYWORDS), 1, 0))
text <- page_content
df <- data.frame(skill = KEYWORDS, count = ifelse(str_detect(text, KEYWORDS), 1, 0))
res$running$count <- res$running$count + df$count
# libraries ---------------------------------------------------------------
library(tidyverse)
files <- list.files(path = "results/words/", full.names = TRUE)
files
files <- list.files(path = "results/words", full.names = TRUE)
files
allwords <-
read_csv(files) %>%
map_df()
ReadMyDataFiles <- function(pattern = pattern, path = "./") {
map(list.files(path = path, pattern = pattern, full.names = TRUE),
read_csv)
}
safely_ReadMyDataFiles <- safely(ReadMyDataFiles)
allwords <- safely_ReadMyDataFiles("results/words")
allwords
allwords$result
length(allwords$result)
allwords$result[1]
allwords <- safely_ReadMyDataFiles(pattern = ".csv", path = "results/words")
allwords[1]
allwords$result[1]
allfiles <- check1$results
check1 <- safely_ReadMyDataFiles(pattern = ".csv", path = "results/words")
allfiles <- check1$results
allfiles <-
check1$results %>%
map_df(filter(n > 10))
allfiles <-
check1$results %>%
map_df(., filter(n > 10))
allfiles <-
check1$results[1] %>%
filter(n > 10)
check1$results[1] %>%
filter(n > 10)
# libraries ---------------------------------------------------------------
library(tidyverse)
ReadMyDataFiles <- function(pattern = pattern, path = "./") {
map(list.files(path = path, pattern = pattern, full.names = TRUE),
read_csv)
}
safely_ReadMyDataFiles <- safely(ReadMyDataFiles)
check1 <- safely_ReadMyDataFiles(pattern = ".csv", path = "results/words")
check1$result[1]
allfiles <-
check1$result[1] %>%
filter(n > 10)
allfiles <-
check1$result[1] %>%
as.tibble() %>%
filter(n > 10)
check1$result[1]
check1$result[1] %>%
unlist()
check1$result[1]
check1$result[1] %>% filter(n > 10)
allfiles <-
check1$result[1] %>%
map_df(~(.) %>%
filter(n > 10))
allfiles
allfiles <-
check1$result[1:2] %>%
map_df(~(.) %>%
filter(n > 10))
allfiles
allfiles <-
check1$result[1:2] %>%
map_df(~(.) %>%
filter(n > 6))
allfiles
allfiles <-
check1$result[1:2] %>%
map_df(~(.) %>%
filter(n > 5))
allfiles
allfiles <-
check1$result %>%
map_df(~(.) %>%
filter(n > 5))
allfiles
View(allfiles)
allfilesg <- allfiles %>%
group(word) %>%
summarise(n = sum(n))
allfilesg <- allfiles %>%
group_by(word) %>%
summarise(n = sum(n))
View(allfilesg)
allfilesg <-
allfiles %>%
group_by(word) %>%
summarise(n = sum(n)) %>%
arrange(desc(n))
View(allfilesg)
allfilesg <-
allfiles %>%
group_by(word) %>%
summarise(n = sum(n)) %>%
filter(n > 10) %>%
arrange(desc(n))
View(allfilesg)
allfiles <-
check1$result %>%
map_df(~(.) )#%>%
View(allfiles)
allfilesg <-
allfiles %>%
group_by(word) %>%
summarise(n = sum(n)) %>%
filter(n > 10) %>%
arrange(desc(n))
View(allfilesg)
allfiles <-
check1$result %>%
map_df(~(.) %>%
filter(grepl('1|2|3|4|5|6|7|8|9|0', word)))
View(allfiles)
allfiles <-
check1$result %>%
map_df(~(.) %>%
filter(grepl("[[:digit:]]", word)))
View(allfiles)
allfiles <-
check1$result %>%
map_df(~(.) %>%
filter(!grepl("[[:digit:]]", word)))
View(allfiles)
# after checking remove the big list to free space
rm(check1)
allfilesg <-
allfiles %>%
group_by(word) %>%
summarise(n = sum(n)) %>%
filter(n > 10) %>%
arrange(desc(n))
View(allfilesg)
View(allfilesg)
allfiles %>%
filter(word == "women")
source('~/Documents/RProjects/jobsScrapping/scripts/allwords.R', echo=TRUE)
allfiles %>%
filter(word == "woman")
View(allfilesg)
# libraries ---------------------------------------------------------------
library(tidyverse)
ReadMyDataFiles <- function(pattern = pattern, path = "./") {
map(list.files(path = path, pattern = pattern, full.names = TRUE),
read_csv)
}
safely_ReadMyDataFiles <- safely(ReadMyDataFiles)
check1 <- safely_ReadMyDataFiles(pattern = ".csv", path = "results/sentences/")
check1$result[1]
allsentences <-
check1$result %>%
map_df(~(.))
View(allsentences)
rm(check1)
allsentences %>%
arrange(sentence) %>%
View()
allsentences[151,]
allsentences[152,]
mysentences
mysentences <-
allsentences %>%
arrange(sentence) %>%
group_by(sentence) %>%
count(sort = TRUE) %>%
ungroup()
mysentences
View(mysentences)
write_csv(mysentences, paste0("../results/", "top_sentences.csv"))
write_csv(mysentences, paste0("results/", "top_sentences.csv"))
library(wordcloud)
mysentences %>%
anti_join(stop_words) %>%
count(sentence) %>%
with(wordcloud(sentence, n, max.words = 100))
library(wordcloud)
library(tidytext)
mysentences %>%
anti_join(stop_words) %>%
count(sentence) %>%
with(wordcloud(sentence, n, max.words = 100))
tidy_books
library(janeaustenr)
tidy_books
tidy_books <- austen_books()
tidy_books
tidy_books %>%
anti_join(stop_words)
tidy_books %>%
anti_join(stop_words, text)
tidy_books <- tidy_books %>%
anti_join(get_stopwords())
library(tidytext)
library(janeaustenr)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(line = row_number()) %>%
ungroup()
tidy_books <- original_books %>%
unnest_tokens(word, text)
tidy_books
stop_words
mysentences %>%
anti_join(stop_words, by = c("word"))
mysentences
mysentences %>%
anti_join(stop_words, by = c("sentence"))
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word"))
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
count(sentence) %>%
with(wordcloud(sentence, n, max.words = 100))
wordcloud()
?wordcloud()
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
count(sentence)
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word"))
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
count(freqs = sentence) %>%
with(wordcloud(sentence, freqs, max.words = 100))
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
count(freqs = sentence)
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word"))
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
mutate(n = "repetitions")
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
rename(n = "repetitions")
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
rename(n = repetitions)
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
rename(c("n" = "repetitions"))
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
select(n = repetitions)
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
select(n = "repetitions")
mysentences %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
select(sentences, n = repetitions)
mysentences %>%
select(sentences, n = repetitions)
mysentences
mysentences %>%
select(sentence, n = repetitions)
mysentences %>%
select(sentence, n = "repetitions")
mysentences %>%
rename("sentence","repetitions")
mysentences %>%
rename(c("sentence","repetitions"))
mysentences %>%
rename(c("sentence","repetitions"))
?rename
# libraries ---------------------------------------------------------------
library(tidyverse)
mysentences %>%
select(sentence, n = repetitions)
mysentences %>%
rename(sentence, n = repetitions)
mysentences %>%
select(sentence, repetitions = n)
mysentences %>%
select(sentence, repetitions = n) %>%
anti_join(stop_words, by = c("sentence" = "word")) %>%
%>%
count(freqs = sentence)
mysentences %>%
select(sentence, repetitions = n) %>%
anti_join(stop_words, by = c("sentence" = "word"))
mysentences %>%
select(sentence, repetitions = n) %>%
#anti_join(stop_words, by = c("sentence" = "word")) %>%
count(sentence)
mysentences %>%
select(sentence, repetitions = n) %>%
#anti_join(stop_words, by = c("sentence" = "word")) %>%
count(sentence) %>%
with(wordcloud(sentence, freqs, max.words = 100))
mysentences %>%
select(sentence, repetitions = n) %>%
#anti_join(stop_words, by = c("sentence" = "word")) %>%
count(sentence) %>%
with(wordcloud(sentence, n, max.words = 100))
mysentences %>%
select(sentence, repetitions = n) %>%
#anti_join(stop_words, by = c("sentence" = "word")) %>%
count(sentence) %>%
arrange(desc(n))
mysentences %>%
select(sentence, repetitions = n) %>%
#anti_join(stop_words, by = c("sentence" = "word")) %>%
count(sentence) %>%
arrange(desc(n)) %>%
with(wordcloud(sentence, n))
mysentences %>%
select(sentence, repetitions = n) %>%
#anti_join(stop_words, by = c("sentence" = "word")) %>%
count(sentence) %>%
arrange(desc(n)) %>%
with(wordcloud(sentence, n))
mysentences %>%
select(sentence, repetitions = n) %>%
#anti_join(stop_words, by = c("sentence" = "word")) %>%
count(sentence) %>%
arrange(desc(n)) %>%
with(wordcloud(sentence, n))
