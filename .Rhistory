links <- new_page %>%
html_nodes("h2 a") %>%
html_attr("href")
# Scrap job links
results <- ScrapeJobLinks(results, links)
}
}
results
# running total
results$running <-
results$running %>%
arrange(desc(count))
print(results$running)
# running total count as percentage
results$running$count <- results$running$count / results$num_jobs
# Reformat the Job Title and Location to readable form
jt <- str_replace_all(job_title, '\\+|\\\"', " ")
loc <- str_replace_all(location, "\\%2C+|\\+", " ")
jt
loc
results$running %>%
mutate(skill = fct_reorder(skill, count))
# Visualization
results$running %>%
mutate(skill = fct_reorder(skill, count)) %>%
ggplot(aes(x = skill, y = count)) +
geom_col(fill = "chocolate2") +
labs(x = "Skill", y = "Occurrences (%)",
title = paste0("Skill occurrences(%) for ", jt, " in ", loc)) +
#scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1))
theme_minimal() +
theme(axis.text.x = element_text(angle = 60, hjust = 1))
# Visualization
results$running %>%
mutate(skill = fct_reorder(skill, -count)) %>%
ggplot(aes(x = skill, y = count)) +
geom_col(fill = "chocolate2") +
labs(x = "Skill", y = "Occurrences (%)",
title = paste0("Skill occurrences(%) for ", jt, " in ", loc)) +
#scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1))
theme_minimal() +
theme(axis.text.x = element_text(angle = 60, hjust = 1))
KEYWORDS <- c("creation", "create", "created", "communicate", "communicating",
"communication", "manage", "manages", "management", "SAS",
"Excel", "AWS", "Azure", "Java", "Tableau", "SQL", "data bases",
"database", "harvests", "improvement", "KPI", "control", "MSc",
"PhD", "experience", "tools", "systems", "autonomous", "methodical",
"organized", "organised", "Strong analytical",
"interpersonal skills", "adapts", "challenge", "challenges",
"continuous", "data management", "programming", "R", "Python")
# Given running total dataframe and links to scrape skills and compute running total
ScrapeJobLinks <- function(res, job_links) {
for (i in 1:length(job_links)) {
job_url <- paste0(base_URL, job_links[i])
Sys.sleep(1)
cat(paste0("Reading job ", i, "\n"))
tryCatch({
html <- read_html(job_url)
text <- html_text(html)
text <- clean.text(text)
df <- data.frame(skill = KEYWORDS, count = ifelse(str_detect(text, KEYWORDS), 1, 0))
res$running$count <- res$running$count + df$count
res$num_jobs <- res$num_jobs + 1
}, error = function(e) {
cat("ERROR :", conditionMessage(e), "\n")
})
}
return(res)
}
# Create running total dataframe
running <- data.frame(skill = KEYWORDS, count = rep(0, length(KEYWORDS)))
# Since the indeed only display max of 20 pages from search result, we cannot use job_count but need to track by creating a num_jobs
num_jobs <- 0
# Here is our results object that contains the two stats
results <- list("running" = running, "num_jobs" = num_jobs)
if (job_count != 0) {
cat("Scraping jobs in Start Page\n")
results <- ScrapeJobLinks(results, links)
}
# running total
results$running <-
results$running %>%
arrange(desc(count))
print(results$running)
# running total count as percentage
results$running$count <- results$running$count / results$num_jobs
# Reformat the Job Title and Location to readable form
jt <- str_replace_all(job_title, '\\+|\\\"', " ")
loc <- str_replace_all(location, "\\%2C+|\\+", " ")
# Visualization
results$running %>%
mutate(skill = fct_reorder(skill, -count)) %>%
ggplot(aes(x = skill, y = count)) +
geom_col(fill = "chocolate2") +
labs(x = "Skill", y = "Occurrences (%)",
title = paste0("Skill occurrences(%) for ", jt, " in ", loc)) +
#scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, 0.1))
theme_minimal() +
theme(axis.text.x = element_text(angle = 60, hjust = 1))
# running total
results$running <-
results$running %>%
arrange(desc(count))
results$running %>%
knitr::kable(caption = "Table 1: Skills per count")
html
job_url
job_link
job_links
results
running
running <- data.frame(skill = KEYWORDS, count = rep(0, length(KEYWORDS)))
running
library(XML)
library(RCurl)
links
base_URL
styler:::style_active_file()
url_link <- "https://www.indeed.com/rc/clk?jk=788e7e311656fc54&fccid=09fad757f3449fa5&vjs=3"
blog <- getURL(url_link)
blog <- htmlParse(blog, encoding = "UTF-8")
titles <- xpathSApply(blog, "//loc", xmlValue)
titles
jobsearch-ViewJobLayout-jobDisplay icl-Grid-col icl-u-xs-span12 icl-u-lg-span7
traverse_each_page <- function(x){
tmp <- htmlParse(getURI(x))
xpathSApply(tmp, '//div[@id="jobsearch-ViewJobLayout-jobDisplay"]', xmlValue)
}
pages <- sapply(titles[2:3], traverse_each_page)
titles
blog
url_link <- "https://us.conv.indeed.com/rc/clk?jk=788e7e311656fc54&amp;ctk=1cm5l21ia5mgndkh&amp;t=cr&amp;rctype=oth&amp;orgclktk=1cm5l21in5mgnel4&amp;vjs=3&amp;wwwho=4m_xAU4HGQbAlJdZTkhlA8hEG_rFObn1"
blog <- getURL(url_link)
blog <- htmlParse(blog, encoding = "UTF-8")
titles <- xpathSApply(blog, "//loc", xmlValue)
titles
blog
url_link <- "https://www.indeed.com/rc/clk?jk=788e7e311656fc54&fccid=09fad757f3449fa5&vjs=3"
blog <- getURL(url_link)
blog
tmp <- htmlParse(getURI("https://www.indeed.com/rc/clk?jk=788e7e311656fc54&fccid=09fad757f3449fa5&vjs=3"))
tmp
xpathSApply(tmp, '//div[@id="jobsearch-ViewJobLayout-jobDisplay"]', xmlValue)
tmp <- htmlParse(getURI("https://jobs.tdameritrade.com/job/-/-/1121/8389790"))
xpathSApply(tmp, '//div[@id="descrip-socialshare"]', xmlValue)
tmp
xpathSApply(tmp, '//div[@class="descrip-socialshare"]', xmlValue)
content <- xpathSApply(tmp, '//div[@class="descrip-socialshare"]', xmlValue)
content
nont <- c("\n", "\t", "\r")
pages <- gsub(paste(nont, collapse = "|"), " ", content)
pages
require(tm)
# convert list into corpus
mycorpus <- Corpus(VectorSource(pages))
# prepare to remove stopwords, ie. common words like 'the'
skipWords <- function(x) removeWords(x, stopwords("english"))
# prepare to remove other bits we usually don't care about
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
# do it
a <- tm_map(mycorpus, FUN = tm_reduce, tmFuns = funcs)
# make document term matrix
mydtm <- DocumentTermMatrix(a, control = list(wordLengths = c(3,10)))
mydtm
inspect(mydtm)
# you can assign it to a data frame for more convenient viewing
my_df <- inspect(mydtm)
my_df
apply(mydtm, 2, sum)
sort(apply(mydtm, 2, sum))
library(tidytext)
library(janeaustenr)
library(dplyr)
url_link <- "https://www.indeed.com/rc/clk?jk=788e7e311656fc54&fccid=09fad757f3449fa5&vjs=3"
url_link
blog <- getURL(url_link)
blog
newURL <- grep("HREF=*", blog)
newURL
str_split(pattern = "\")
blog %>%
str_split(pattern = "\\")
str_split(pattern = "\\"\")
blog %>%
str_split(pattern = "[\]")
blog %>%
str_split(pattern = "[\]")
new URL <- "https://us.conv.indeed.com/rc/clk?jk=788e7e311656fc54&ctk=1cm5lu3h64esg8oo&t=cr&rctype=oth&orgclktk=1cm5lu3hm4esg9u5&vjs=3&wwwho=4m_xAU4HGQbAlJdZTkhlA8hEG_rFObn1"
blog <- getURL(new_URL)
new_URL <- "https://us.conv.indeed.com/rc/clk?jk=788e7e311656fc54&ctk=1cm5lu3h64esg8oo&t=cr&rctype=oth&orgclktk=1cm5lu3hm4esg9u5&vjs=3&wwwho=4m_xAU4HGQbAlJdZTkhlA8hEG_rFObn1"
blog <- getURL(new_URL)
blog
url_link <- "https://www.indeed.com/rc/clk?jk=788e7e311656fc54&fccid=09fad757f3449fa5&vjs=3"
blog <- getURL(url_link)
blog
new_URL <- "https://us.conv.indeed.com/rc/clk?jk=788e7e311656fc54&ctk=1cm5m4d0a0m6k49v&t=cr&rctype=oth&orgclktk=1cm5m4d0o0m6k1g6&vjs=3&wwwho=4m_xAU4HGQbAlJdZTkhlA8hEG_rFObn1"
blog <- getURL(new_URL)
blog
tmp <- htmlParse(getURI("https://jobs.tdameritrade.com/job/-/-/1121/8389790"))
content <- xpathSApply(tmp, '//div[@class="descrip-socialshare"]', xmlValue)
nont <- c("\n", "\t", "\r")
pages <- gsub(paste(nont, collapse = "|"), " ", content)
pages
tidy_pages <- pages %>%
unnest_tokens(word, text)
tidy_pages <- pages %>%
as_tibble() %>%
unnest_tokens(word, text)
tidy_pa
tidy_pages
austen_books()
tidy_pages <- pages %>%
as_tibble(text = pages) %>%
unnest_tokens(word, text)
tidy_pages
pages %>%
as_tibble(text)
pages %>%
as_tibble(text = value)
pages %>%
tbl()
pages
pages %>%
bind_rows()
bind_rows(pages)
bind_rows(pages, "text)
bind_rows(pages, "text")
bind_rows(pages, .id = "text")
library(tidyverse)
pages %>%
map_df() %>%
as_tibble()
pages %>%
map_df(.) %>%
as_tibble()
pages %>%
map_df(~t(.) %>%
as_tibble)
content <-
pages %>%
map_df(~t(.) %>%
as_tibble)
content
tidy_books <- content %>%
unnest_tokens(word, text)
tidy_books
content
content <-
pages %>%
map_df(~(.) %>%
as_tibble)
content
content <-
pages %>%
map_df(~(.) %>% as_tibble) %>%
mutate(text = value)
content
content <-
pages %>%
map_df(~(.) %>% as_tibble) %>%
mutate(value = text)
text =
text =
content <-
pages %>%
map_df(~(.) %>% as_tibble) %>%
mutate(text = value)
content
content <-
pages %>%
map_df(~(.) %>% as_tibble)
content
View(content)
content$value
tidy_books <- content %>%
unnest_tokens(word, text)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(line = row_number()) %>%
ungroup()
original_books
tidy_books <- original_books %>%
unnest_tokens(word, text)
tidy_books
content <-
pages %>%
map_df(~(.) %>% as_tibble)
content
colnames(content)
colnames(content) <- c("doc_id", "text")
colnames(content) <-  "text"
content
content %>%
unnest_tokens(word, text)
tidy_word <-
content %>%
unnest_tokens(word, text)
tidy_word
tidy_word %>%
count(word, sort = TRUE)
tidy_word <- tidy_word %>%
anti_join(get_stopwords())
tidy_word %>%
count(word, sort = TRUE)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(book, index = line %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
get_sentiments("bing")
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(book, index = line %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
#count(book, index = line %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(word, index = line %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word")
janeaustensentiment
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
#count(word, index = line %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
#count(word, index = line %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0)
tidy_word
tidy_word <- tidy_word %>%
mutate(doc = "doc1")
tidy_word
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(doc, index = line %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
tidy_word <- tidy_word %>%
mutate(doc = factor("doc1"))
tidy_word
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(doc, index = line %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
get_sentiments("bing")
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
count(doc, index = line %/% 80, sentiment)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
# count(doc, index = line %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
janeaustensentiment
ggplot(janeaustensentiment, aes(index, sentiment, fill = doc)) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~doc, ncol = 2, scales = "free_x")
ggplot(tidy_word, aes(index, sentiment, fill = doc)) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~doc, ncol = 2, scales = "free_x")
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word")
janeaustensentiment
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
spread(sentiment, n)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
spread(sentiment)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
spread(key = sentiment, value = n)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
spread(key = sentiment, value = index)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word")
janeaustensentiment
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
spread(sentiment, word )
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
spread(word, sentiment)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
spread(sentiment, doc)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
spread(sentiment)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word") %>%
spread(sentiment, fill = 0)
janeaustensentiment <- tidy_word %>%
inner_join(get_sentiments("bing"), by = "word")
janeaustensentiment
ggplot(janeaustensentiment, aes(index, sentiment, colour = sentiment)) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~doc, ncol = 2, scales = "free_x")
tidy_word %>%
count(word, sort = TRUE)
content <-
pages %>%
map_df(~(.) %>% as_tibble)
content
colnames(content) <-  "text"
content
tidy_word <-
content %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords())
tidy_word %>%
count(word, sort = TRUE)
tidy_word <-
content %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE)
tidy_word <-
content %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords())
content <-
pages %>%
map_df(~(.) %>% as_tibble)
content
colnames(content) <-  "text"
content
content
content
content <-
pages %>%
map_df(~(.) %>% as_tibble)
content
colnames(content) <-  "words"
content
tidy_word <-
content %>%
unnest_tokens(word, text) %>%
anti_join(get_stopwords()) %>%
count(words, sort = TRUE)
tidy_word
nont <- c("\n", "\t", "\r")
pages <- gsub(paste(nont, collapse = "|"), " ", content)
pages
tidy_word <-
content %>%
unnest_tokens(word, text)
content <-
pages %>%
map_df(~(.) %>% as_tibble)
content
colnames(content) <-  "words"
content
tidy_word <-
content %>%
unnest_tokens(word, text)
pages <- gsub(paste(nont, collapse = "|"), " ", content)
content <-
pages %>%
map_df(~(.) %>% as_tibble)
content
colnames(content) <-  "words"
content
tidy_word <-
content %>%
unnest_tokens(word, text)
tidy_word <-
content %>%
unnest_tokens(word, words)
tidy_word
tidy_word <-
content %>%
unnest_tokens(word, words) %>%
anti_join(get_stopwords()) %>%
count(words, sort = TRUE)
tidy_word <-
content %>%
unnest_tokens(word, words) %>%
anti_join(get_stopwords()) %>%
count(word, sort = TRUE)
tidy_word
content <-
pages %>%
map_df(~(.) %>% as_tibble)
content
colnames(content) <-  "text"
content
content %>%
unnest_tokens(word, text) %>%
anti_join(stop_words) %>%
group_by(word) %>%
count(sort = TRUE) %>%
ungroup() %>%
top_n(10) %>%
ggplot(aes(x = fct_reorder(word, n), y = n)) +
geom_bar(stat = "identity", width = 0.5) +
xlab(NULL) +
coord_flip() +
ylab("Word Frequency") +
ggtitle("Most Common Corpus Words") +
theme(legend.position = "none")
